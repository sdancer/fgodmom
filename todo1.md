### the problem of too much "human thinking"

the agents needs not to bother with the concept of "too much work" or "too much complexity"
it needs to make plans that allow for human non doable amounts of work
example, transcopiling functions one by one "by hand" is doable for an llm, particularly if the function count is 500 or so, while its months or weeks of work for a human, an llm can do in 3-5 minutes
quote> Ghidra can even decompile this further into pseudo-C, which is incredibly helpful for understanding logic.
another problem described here, is the tendency to reduce/simplify stuff, where still unnecesary, that is indeed very human, but hides important details and the llm can deal with them piece by piece without problem

- so, what are we missing out?
asking for context before formulating the plan
iteratively scoping in and settling facts
building scafolding for the process

in sum, still giving us a human-executable plan, not a high level logistics execution plan to refine a search on the space of possibilities

good plan, but didn't ask anything :P
so good plan to do the wrong thing

probably we can use reinforcement learning over the prompt, with like 1k-10k until the system gets to ask us what game before making the plan xD

most likely this is a quite-soon solved problem, in the sense that, everyone is trying to use agents, they gotta figure the solution before agents are useful for real.


